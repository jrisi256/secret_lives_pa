{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76b1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d676522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF.\n",
    "# Argument pdf_path is of type string.\n",
    "# Returns a string.\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    # Open up the PDF and read in each page.\n",
    "    pages = pdfplumber.open(pdf_path).pages\n",
    "    # List comprehension. Structure is expression FOR x IN y.\n",
    "    # Execute expression on each x in y.\n",
    "    # Here we extract the text from each page. Then we separate each text element with the new line character.\n",
    "    alltext = \"\\n\".join([page.extract_text(keep_blank_chars=True, layout=True) for page in pages])\n",
    "    return alltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843feaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracts sections from the document text.\\n\\n    Args:\\n        text (str): The text of the document.\\n\\n    Returns:\\n        dict: A dictionary containing the extracted sections with the section headers as keys.\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_sections(text) -> dict[str, str]:\n",
    "    # Find lines that contain 4 or more upper-case characters and/or slashes and/or hyphens (and are bookended by white space).\n",
    "    # These will be the section headers.\n",
    "    section_header_pattern = re.compile(r\"^\\s*[A-Z\\s\\/\\-]{4,}\\s*$\", re.MULTILINE)\n",
    "\n",
    "    # Find all section headers and their starting character index.\n",
    "    matches = re.finditer(section_header_pattern, text)\n",
    "\n",
    "    # Iterate through each match and find the starting character index as well as the section header.\n",
    "    headers = [(match.start(), match.group().strip()) for match in matches]\n",
    "    # Drop any potential headers that are just empty whitespace.\n",
    "    headers = [h for h in headers if len(h[1]) > 0]\n",
    "\n",
    "    # Dictionary to store sections\n",
    "    sections = {}\n",
    "\n",
    "    # Iterate over headers and extract sections.\n",
    "    for i in range(len(headers)):\n",
    "        start_index = headers[i][0]\n",
    "        header = headers[i][1]\n",
    "        # Set the end index to be the start index of the next section header (or the end of the text file).\n",
    "        end_index = headers[i + 1][0] if i + 1 < len(headers) else len(text)\n",
    "\n",
    "        # Extract section text.\n",
    "        section_text = text[start_index:end_index].strip()\n",
    "\n",
    "        # Remove the header from the section text.\n",
    "        section_text = section_text[len(header):].strip()\n",
    "\n",
    "        # Reduce different versions of the same header to a single version\n",
    "        if \"ATTORNEY INFORMATION\" in header:\n",
    "            header = \"ATTORNEY INFORMATION\"\n",
    "        elif \"BAIL INFORMATION\" in header:\n",
    "            header = \"BAIL\"\n",
    "\n",
    "        # Add the current section header to our dictionary of sections.\n",
    "        sections.setdefault(header, \"\")\n",
    "\n",
    "        # TO DO PICK UP HERE ###############################################################################\n",
    "        sections[header] += f\"\\n{section_text}\"\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "\"\"\"Extracts sections from the document text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text of the document.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted sections with the section headers as keys.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fb118e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all(pdf_path: str) -> dict[str, str | dict]:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    sections = extract_sections(text)\n",
    "    return(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a748863",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = extract_all(\"../output/pdf_sample/pdfs/ds_Allegheny_CP_02_CR_0000033_2019.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secret_lives_pa",
   "language": "python",
   "name": "secret_lives_pa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
