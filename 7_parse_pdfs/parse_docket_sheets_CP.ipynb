{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF.\n",
    "# Args:\n",
    "#   pdf_path (str): File path to the PDF.\n",
    "# Returns:\n",
    "#   string: A single string which is the concatenated version of all pages and lines in the PDF.\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    # Open up the PDF and read in each page.\n",
    "    pages = pdfplumber.open(pdf_path).pages\n",
    "    # List comprehension. Structure is expression FOR x IN y.\n",
    "    # Execute expression on each x in y.\n",
    "    # Here we extract the text from each page. Then we separate each text element with the new line character.\n",
    "    alltext = \"\\n\".join([page.extract_text(keep_blank_chars=True, layout=True) for page in pages])\n",
    "    return alltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracts sections from the document text.\n",
    "# Args:\n",
    "#   text (str): The text of the document.\n",
    "# Returns:\n",
    "#   dict: A dictionary containing the extracted sections with the section headers as keys.\n",
    "\n",
    "def extract_sections(text: str) -> dict[str, str]:\n",
    "    # Find lines that contain 4 or more upper-case characters and/or slashes and/or hyphens (and are bookended by white space).\n",
    "    # These will be the section headers.\n",
    "    section_header_pattern = re.compile(r\"^\\s*[A-Z\\s\\/\\-]{4,}\\s*$\", re.MULTILINE)\n",
    "\n",
    "    # Find all section headers and their starting character index.\n",
    "    matches = re.finditer(section_header_pattern, text)\n",
    "\n",
    "    # Iterate through each match and find the starting character index as well as the section header.\n",
    "    headers = [(match.start(), match.group().strip()) for match in matches]\n",
    "    # Drop any potential headers that are just empty whitespace.\n",
    "    headers = [h for h in headers if len(h[1]) > 0]\n",
    "\n",
    "    # Dictionary to store sections\n",
    "    sections = {}\n",
    "\n",
    "    # Iterate over headers and extract sections.\n",
    "    for i in range(len(headers)):\n",
    "        start_index = headers[i][0]\n",
    "        header = headers[i][1]\n",
    "        # Set the end index to be the start index of the next section header (or the end of the text file).\n",
    "        end_index = headers[i + 1][0] if i + 1 < len(headers) else len(text)\n",
    "\n",
    "        # Extract section text.\n",
    "        section_text = text[start_index:end_index].strip()\n",
    "\n",
    "        # Remove the header from the section text.\n",
    "        section_text = section_text[len(header):].strip()\n",
    "\n",
    "        # Reduce different versions of the same header to a single version\n",
    "        if \"ATTORNEY INFORMATION\" in header:\n",
    "            header = \"ATTORNEY INFORMATION\"\n",
    "        elif \"BAIL INFORMATION\" in header:\n",
    "            header = \"BAIL\"\n",
    "\n",
    "        # Add the current section header to our dictionary of sections.\n",
    "        # setdefault searches for the key in your dictionary if it exists.\n",
    "        # If it does exist, it returns the value associated with the key. If it does not exist, the key is inserted with the provided default value.\n",
    "        sections.setdefault(header, \"\")\n",
    "\n",
    "        # Add the section text to the dictionary under the header key.\n",
    "        sections[header] += f\"\\n{section_text}\"\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fe906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the defendant's information from the DEFENDANT INFORMATION section.\n",
    "# Args:\n",
    "#   text (str): The text containing the defendant's information.\n",
    "# Return:\n",
    "#   dict: A dictionary containing the extracted information.\n",
    "\n",
    "def extract_defendant_information(text: str) -> dict[str, str | list]:\n",
    "    split = text.split(\"\\n\")\n",
    "    extracted_info = {}\n",
    "    i = 0\n",
    "\n",
    "    # Defendant information follows a straightforward pattern.\n",
    "    # In CP dockets, there is only one line which contains defendant DOB and address.\n",
    "    while(i < len(split)):\n",
    "        line = split[i].lower().strip()\n",
    "        if(\"date of birth:\" in line and \"city/state/zip:\" in line):\n",
    "            extracted_info[\"dob\"] = line.split(\"date of birth:\")[1].split(\"city/state/zip:\")[0].strip()\n",
    "            extracted_info[\"address\"] = line.split(\"date of birth:\")[1].split(\"city/state/zip:\")[1].strip()\n",
    "            i += 1\n",
    "        # Line is a junk line. Keep moving on.\n",
    "        else:\n",
    "            i += 1\n",
    "        \n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07f41b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1620509647.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 31\u001b[0;36m\u001b[0m\n\u001b[0;31m    extracted_info[]\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def extract_case_information(text: str) -> dict[str, str | list]:\n",
    "    split = text.split(\"\\n\")\n",
    "    extracted_info = {}\n",
    "    i = 0\n",
    "\n",
    "    # Case information follows a straightforward pattern.\n",
    "    # In CP dockets:\n",
    "    #   Line 1 (Optional and potentially multiple lines) is cross court docket numbers.\n",
    "    #   Line 2 is judge assigned, date filed, and initiation date.\n",
    "    #   Line 3 is OTN, LOTN, and originating docket number.\n",
    "    #   Line 4 is initial and final issuing authority.\n",
    "    #   Line 5 is arresting agency (potentially multiple lines) and arresting officer.\n",
    "    #   Line 6 is complaint/citation number and incident number.\n",
    "    #   Line 7 is county and township.\n",
    "    #   Line 8 + 9 is case local number type and case local number.\n",
    "    while(i < len(split)):\n",
    "        line = split[i].lower().strip()\n",
    "        \n",
    "        if(\"cross court docket nos:\" in line):\n",
    "            extracted_info[\"cross_court_docker_nrs\"] = line.split(\",\")\n",
    "            j = 1\n",
    "\n",
    "            # Check the next line for more docket numbers.\n",
    "            while(\"judge assigned\" not in split[i + j] and \"date filed\" not in split[i + j] and \"initiation date\" not in split[i + j]):\n",
    "                lookahead_line = split[i + j].lower.strip()\n",
    "                extracted_info[\"cross_court_docker_nrs\"] = extracted_info[\"cross_court_docker_nrs\"].extend(lookahead_line.split(\",\"))\n",
    "                j += 1\n",
    "            \n",
    "            i += 1 + (j - 1)\n",
    "        elif(\"judge assigned:\" in line or \"date filed:\" in line or \"initiation date:\" in line):\n",
    "            extracted_info[\"judge\"] = line.split(\"judge assigned:\")[1].split(\"date filed:\")[0].strip()\n",
    "            extracted_info[\"date_filed\"] = line.split(\"judge assigned:\")[1].split(\"date filed:\")[1].split(\"initiation date:\")[0].strip()\n",
    "            extracted_info[\"initiation_date\"] = line.split(\"judge assigned:\")[1].split(\"date filed:\")[1].split(\"initiation date:\")[1].strip()\n",
    "            i += 1\n",
    "        elif(\"otn:\" in line or \"lotn:\" in line or \"originating docket no:\" in line):\n",
    "            extracted_info[\"otn\"] = line.split(\"otn:\")[1].split(\"lotn:\")[0].strip()\n",
    "            extracted_info[\"otn\"] = line.split(\"otn:\")[1].split(\"lotn:\")[1].split(\"originating docket no:\")[0].strip()\n",
    "            extracted_info[\"originating_docket_nr\"] = line.split(\"otn:\")[1].split(\"lotn:\")[1].split(\"originating docket no:\")[1].strip()\n",
    "            i += 1\n",
    "        elif(\"initial issuing authority:\" in line or \"final issuing authority:\" in line):\n",
    "            \n",
    "\n",
    "        # Junk line. Keep moving.\n",
    "        else:\n",
    "            i += 1\n",
    "                \n",
    "\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8fb118e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all(pdf_path: str) -> dict[str, str | dict]:\n",
    "    # Join together all pages and lines into one string.\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Partition the text by sections.\n",
    "    sections = extract_sections(text)\n",
    "\n",
    "    # Extract defendant information.\n",
    "    defendant_info = (\n",
    "        extract_defendant_information(sections.get(\"DEFENDANT INFORMATION\", \"\"))\n",
    "        if \"DEFENDANT INFORMATION\" in sections\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    return(\n",
    "        {\n",
    "            \"defendant_info\": defendant_info\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a748863",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = extract_all(\"../output/pdf_sample/pdfs/ds_Allegheny_CP_02_CR_0000033_2019.pdf\")\n",
    "a6 = extract_all(\"../output/pdf_sample/pdfs/ds_Allegheny_CP_02_CR_0015558_2006.pdf\")\n",
    "a10 = extract_all(\"../output/pdf_sample/pdfs/ds_Montgomery_CP_46_CR_0000333_2019.pdf\")\n",
    "a11 = extract_all(\"../output/pdf_sample/pdfs/ds_Montgomery_CP_46_CR_0001933_2010.pdf\")\n",
    "a13 = extract_all(\"../output/pdf_sample/pdfs/ds_Allegheny_CP_02_CR_0000789_2023.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e507e10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defendant_info': {'dob': '11/21/1998', 'address': 'duquesne, pa  15110'}}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f81ec536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defendant_info': {'name': 'martin, gregg jourdan',\n",
       "  'sex': 'male',\n",
       "  'dob': '09/20/1990',\n",
       "  'race': 'black',\n",
       "  'address_type': ['Home'],\n",
       "  'address': ['Pittsburgh, PA 15217'],\n",
       "  'counsel': 'no',\n",
       "  'defender_requested': 'no',\n",
       "  'application_provided': 'no',\n",
       "  'fingerprinted': 'no'}}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "442637eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defendant_info': {'name': 'nebelski, sandra',\n",
       "  'sex': 'female',\n",
       "  'dob': '10/11/1956',\n",
       "  'race': 'white',\n",
       "  'address_type': ['Home', 'Home'],\n",
       "  'address': ['Altoona, PA 16602', 'Altoona, PA 16602'],\n",
       "  'counsel': 'yes',\n",
       "  'defender_requested': 'yes',\n",
       "  'application_provided': 'yes',\n",
       "  'fingerprinted': 'no'}}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3c22d580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defendant_info': {'name': 'lowery, wayne a.',\n",
       "  'sex': '',\n",
       "  'dob': '07/01/1964',\n",
       "  'race': '',\n",
       "  'address_type': [],\n",
       "  'address': [],\n",
       "  'counsel': 'no',\n",
       "  'defender_requested': 'yes',\n",
       "  'application_provided': 'yes',\n",
       "  'fingerprinted': 'no'}}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "661a8734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defendant_info': {'name': 'murphy, yancey',\n",
       "  'sex': 'male',\n",
       "  'dob': '04/01/1970',\n",
       "  'race': 'black',\n",
       "  'address_type': ['Home'],\n",
       "  'address': [],\n",
       "  'counsel': 'yes',\n",
       "  'defender_requested': 'no',\n",
       "  'application_provided': 'no',\n",
       "  'fingerprinted': 'yes'}}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secret_lives_pa_virtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
