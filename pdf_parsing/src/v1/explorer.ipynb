{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"../../data/example_docketsheets_courtsummaries/CS_MJ-05216-CR-0000076-2022.pdf\"\n",
    "# file_name = '../courtsummary2.pdf'\n",
    "# file_name = \"../../data/example_docketsheets_courtsummaries/CS_MJ-07104-CR-0000001-2023.pdf\"\n",
    "# file_name = \"../../data/example_docketsheets_courtsummaries/CS_MJ-07108-CR-0000086-2020.pdf\"\n",
    "# file_name = \"../../data/example_docketsheets_courtsummaries/CS_MJ-12301-CR-0000168-2017.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../../data/example_docketsheets_courtsummaries/\"\n",
    "files = os.listdir(directory)\n",
    "court_summaries = [f for f in files if 'CS' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CS_CP-05-CR-0000151-2006.pdf',\n",
       " 'CS_MJ-12301-CR-0000168-2017.pdf',\n",
       " 'CS_MJ-07104-CR-0000001-2023.pdf',\n",
       " 'CS_ MJ-17302-CR-0000035-2015.pdf',\n",
       " 'CS_MJ-07108-CR-0000086-2020.pdf',\n",
       " 'CS_MJ-19310-CR-0000176-2011.pdf',\n",
       " 'CS_MJ-26304-CR-0000183-2006.pdf',\n",
       " 'CS_CP-51-CR-0013145-2010.pdf',\n",
       " 'CS_CP-27-CR-0000035-2005.pdf',\n",
       " 'CS_MJ-05216-CR-0000076-2022.pdf']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "court_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_indices(str_to_find,case_info):\n",
    "    occurences = [x for x in case_info if str_to_find in x]\n",
    "    index_occurences = [case_info.index(x) for x in occurences]\n",
    "    return index_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Page 1\n",
      "No Processing Found\n",
      "'case_number'\n",
      "Parsing Page 1\n",
      "Parsing Page 2\n",
      "No Processing Found\n",
      "Parsing Page 3\n",
      "Parsing Page 4\n",
      "Parsing Page 5\n",
      "No Processing Found\n",
      "Parsing Page 6\n",
      "No Processing Found\n",
      "'case_number'\n",
      "Parsing Page 1\n",
      "Parsing Page 1\n",
      "Parsing Page 1\n",
      "Parsing Page 1\n",
      "Parsing Page 1\n",
      "Parsing Page 2\n",
      "No Processing Found\n",
      "'case_number'\n",
      "Parsing Page 1\n",
      "No Processing Found\n",
      "Parsing Page 2\n",
      "No Processing Found\n",
      "Parsing Page 3\n",
      "No Processing Found\n",
      "'case_number'\n",
      "Parsing Page 1\n",
      "No Processing Found\n",
      "Parsing Page 2\n",
      "No Processing Found\n",
      "'case_number'\n",
      "Parsing Page 1\n"
     ]
    }
   ],
   "source": [
    "for cs_file in court_summaries:\n",
    "    full_path = f\"{directory}/{cs_file}\"\n",
    "    pdf = pdfplumber.open(full_path)\n",
    "\n",
    "    pages = pdf.pages\n",
    "\n",
    "    \"\"\"\n",
    "        ∧＿∧\n",
    "       (｡･ω･｡)つ━☆・*。\n",
    "     ⊂/    /       ・゜\n",
    "      しーＪ         °。+ * 。\n",
    "\n",
    "    Parse through each PDF, if any error comes up write it a text file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info_dict = {}\n",
    "        for i,page in enumerate(pages):\n",
    "            print(f\"Parsing Page {i+1}\")\n",
    "            text = page.extract_text(keep_blank_chars=True,layout=True).split(\"\\n\")\n",
    "\n",
    "            # first page with POI\n",
    "            dob_index = [i for i,x in enumerate(text) if \"DOB:\" in x]\n",
    "            closed_indices = [i for i,x in enumerate(text) if \"closed\" in x.lower() or \"inactive\" in x.lower()]\n",
    "            if i == 0:\n",
    "                poi_info = text[dob_index[0]:closed_indices[0]]\n",
    "                # Name / DOB / SEX\n",
    "                poi_info_l0 = poi_info[0].split(\"DOB:\")\n",
    "                info_dict['Name'] = poi_info_l0[0].strip()\n",
    "                poi_info_l00 = poi_info_l0[1].split(\"Sex:\")\n",
    "                info_dict['DOB'] = poi_info_l00[0].strip()\n",
    "                info_dict['Sex'] = poi_info_l00[1].strip()\n",
    "                # LOCATION / BLANK / EYES\n",
    "                poi_info_l1 = poi_info[1].split(\"Eyes:\")\n",
    "                info_dict['Location'] = poi_info_l1[0].strip()\n",
    "                info_dict['Eyes'] = poi_info_l1[1].strip()\n",
    "                # ALIASES (TITLE ) / BLANK /  HAIR\n",
    "                poi_info_l2 = poi_info[2].split(\"Hair:\")\n",
    "                info_dict['Hair'] = poi_info_l1[1].strip()\n",
    "                # # ALIAS / BLANK / RACE\n",
    "                poi_info_l3 = poi_info[3].split(\"Race:\")\n",
    "                info_dict['Aliases'] = poi_info_l3[0].strip()\n",
    "                info_dict['Race'] = poi_info_l3[1].strip()\n",
    "                if len(poi_info) > 4:\n",
    "                    # sometimes empty strings or Court information is parsed from this Alias strategy\n",
    "                    # sometimes Aliases: Name will be included\n",
    "                    pre_cleaned_aliases = [x.strip().split(\":\")[-1] for x in [info_dict[\"Aliases\"]] + poi_info[4:] if 'Court' not in x]\n",
    "                    info_dict[\"Aliases\"] = [x for x in pre_cleaned_aliases if len(x)>2]\n",
    "\n",
    "\n",
    "            case_info = text[closed_indices[0]+1:]\n",
    "            closed_endings = [0] + give_indices('Proc',case_info)\n",
    "            printed_endings = give_indices('Printed',case_info)\n",
    "\n",
    "            start_index = 0\n",
    "            if len(closed_endings) < 2:\n",
    "                closed_endings.extend(give_indices('Recent entries',case_info))\n",
    "            end_index = closed_endings[1]\n",
    "\n",
    "            case_numbers = []\n",
    "            case_information = []\n",
    "            while len(closed_endings) > 0:\n",
    "            # processing cases \n",
    "                \n",
    "                start_index = closed_endings[0]\n",
    "                if len(closed_endings) > 1:\n",
    "                    end_index = closed_endings[1]-1\n",
    "                else:\n",
    "                    end_index = printed_endings[0]\n",
    "\n",
    "                case_sliced = case_info[start_index:end_index]\n",
    "\n",
    "                    ## ripping out case number\n",
    "                # '        MJ-26303-CR-0000170-2003 Processing Status: Completed OTN: H 647213-0       ',\n",
    "                processing_status_str = 'Processing Status:'\n",
    "                proc_status_str = 'Proc Status:'\n",
    "\n",
    "                if any(filter(lambda s: processing_status_str in s, case_sliced)):\n",
    "                    case_number_str = [x for x in case_sliced if processing_status_str in x]\n",
    "                    case_number_index = give_indices(processing_status_str,case_number_str)[0]+1\n",
    "                    case_number = case_number_str[0].split(processing_status_str)[0].strip()\n",
    "                    \n",
    "                elif any(filter(lambda s: proc_status_str in s, case_sliced)):\n",
    "                    case_number_str = [x for x in case_sliced if proc_status_str in x]\n",
    "                    case_number_index = give_indices(proc_status_str,case_number_str)[0]+1\n",
    "                    case_number = case_number_str[0].split(proc_status_str)[0].strip()\n",
    "                else:\n",
    "                    print(\"No Processing Found\")\n",
    "                    break\n",
    "                \n",
    "\n",
    "                # continue processing the rest of the case info\n",
    "                case_sliced = case_sliced[case_number_index:]\n",
    "\n",
    "                \n",
    "\n",
    "                #  '        Statute          Grade Description             Disposition          Counts  ',\n",
    "                column_names_cases = give_indices('Statute',case_sliced)[0]+1\n",
    "\n",
    "                # Hacky Way to get Description Column\n",
    "                # 1. Determine at what index Description occurs\n",
    "                description_index = case_sliced[column_names_cases-1].index(\"Description\")\n",
    "                \n",
    "\n",
    "                case_info_dict = {}\n",
    "                sliced_case_v0 = case_sliced[:column_names_cases-1]\n",
    "\n",
    "                for x in sliced_case_v0:\n",
    "                    pre_split_v0 = x.split(\"  \")\n",
    "                    pre_split_ws_removed_v0 = [s for s in pre_split_v0 if s]\n",
    "                    for val in pre_split_ws_removed_v0:\n",
    "                        vals = val.split(\":\")\n",
    "                        if len(vals) > 1:\n",
    "                            case_info_dict[vals[0].strip()] = vals[1]\n",
    "\n",
    "                sliced_case_v1 = case_sliced[column_names_cases:]\n",
    "\n",
    "                stat = []\n",
    "                gd = []\n",
    "                counts = []\n",
    "                description = []\n",
    "                disposition = []\n",
    "                for x in sliced_case_v1:\n",
    "                    pre_split = x.split(\"  \")\n",
    "                    pre_split_ws_removed = [s for s in pre_split if s]\n",
    "                    if len(pre_split_ws_removed) > 3:\n",
    "                        stat.append(pre_split_ws_removed[0].strip())\n",
    "                        gd.append(pre_split_ws_removed[1].strip())\n",
    "                        # Descriptions generally follow the same place as the Column\n",
    "                        description.append(pre_split_ws_removed[2].strip())\n",
    "                        disposition.append(pre_split_ws_removed[3].strip())\n",
    "                        counts.append(pre_split_ws_removed[-1].strip())\n",
    "                        \n",
    "                case_info_dict['Statute'] = stat\n",
    "                case_info_dict['Grade Description'] = gd\n",
    "                case_info_dict['Counts'] = counts\n",
    "                case_info_dict['Description'] = description\n",
    "                case_info_dict['Disposition'] = disposition\n",
    "                case_info_dict['Status'] = text[closed_indices[0]].strip()\n",
    "                case_info_dict['case_number'] = case_number\n",
    "\n",
    "                # info_dict[case_number] = case_info_dict\n",
    "                case_numbers.append(case_number)\n",
    "                case_information.append(case_info_dict)\n",
    "\n",
    "                closed_endings.pop(0)\n",
    "\n",
    "        \"\"\"\n",
    "        Construct a POI table where POI information is repeated for each unique case\n",
    "        \"\"\"\n",
    "        df_info = pd.json_normalize(info_dict)\n",
    "        df_case = pd.DataFrame(np.tile(df_info,(len(set(case_numbers)),1)))\n",
    "        df_case['case_number'] = list(set(case_numbers))\n",
    "\n",
    "        \"\"\"\n",
    "        Merge POI table with case information\n",
    "        \"\"\"\n",
    "\n",
    "        df_final = pd.merge(df_case,pd.DataFrame(case_information),how='outer',left_on='case_number',right_on='case_number')\n",
    "        \n",
    "        file_name = cs_file.replace(\".pdf\",\"\")\n",
    "        df_final.to_csv(f\"data/{file_name}.csv\")\n",
    "\n",
    "\n",
    "        with open(\"parsed_pdfs.txt\", \"a\") as file:\n",
    "            file.write(cs_file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        with open(\"tricky_pdfs.txt\", \"a\") as file:\n",
    "            file.write(cs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
