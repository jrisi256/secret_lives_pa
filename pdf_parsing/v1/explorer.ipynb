{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/example_docketsheets_courtsummaries/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m*ฅ^•ﻌ•^ฅ*\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mInput and Output Directory\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m*ฅ^•ﻌ•^ฅ*\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/example_docketsheets_courtsummaries/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming Court summaries have the following file format -> CS_ MJ-17302-CR-0000035-2015.pdf\u001b[39;00m\n\u001b[1;32m     10\u001b[0m court_summaries \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/example_docketsheets_courtsummaries/'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "*ฅ^•ﻌ•^ฅ*\n",
    "Input and Output Directory\n",
    "*ฅ^•ﻌ•^ฅ*\n",
    "\"\"\"\n",
    "\n",
    "input_directory = \"../data/example_docketsheets_courtsummaries/\"\n",
    "files = os.listdir(input_directory)\n",
    "# Assuming Court summaries have the following file format -> CS_ MJ-17302-CR-0000035-2015.pdf\n",
    "court_summaries = [f for f in files if 'CS' in f]\n",
    "\n",
    "output_directory = \"parsed_court_summaries\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_indices(str_to_find,case_info):\n",
    "    occurences = [x for x in case_info if str_to_find in x]\n",
    "    index_occurences = [case_info.index(x) for x in occurences]\n",
    "    return index_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 files took 2.873115062713623 seconds to process\n",
      "5 (0.500000%) were successfully extracted\n",
      "5 (0.500000%) were not succesfully extracted\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "*ฅ^•ﻌ•^ฅ*\n",
    "Big Loop to process a Court Summary page by page. \n",
    "*ฅ^•ﻌ•^ฅ*\n",
    "\"\"\"\n",
    "# stats for file processing\n",
    "start_time = time.time()\n",
    "# keeping track of files processed\n",
    "good_files_counter = 0\n",
    "tricky_files_counter = 0\n",
    "\n",
    "for cs_file in court_summaries:\n",
    "\n",
    "    full_path = f\"{input_directory}/{cs_file}\"\n",
    "    pdf = pdfplumber.open(full_path)\n",
    "\n",
    "    pages = pdf.pages\n",
    "\n",
    "    \"\"\"\n",
    "        ∧＿∧\n",
    "       (｡･ω･｡)つ━☆・*。\n",
    "     ⊂/    /       ・゜\n",
    "      しーＪ         °。+ * 。\n",
    "\n",
    "    Parse through each PDF, if any error comes up write it a text file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info_dict = {}\n",
    "        for i,page in enumerate(pages):\n",
    "            # print(f\"Parsing Page {i+1}\")\n",
    "            text = page.extract_text(keep_blank_chars=True,layout=True).split(\"\\n\")\n",
    "\n",
    "            # first page with POI\n",
    "            dob_index = [i for i,x in enumerate(text) if \"DOB:\" in x]\n",
    "            closed_indices = [i for i,x in enumerate(text) if \"closed\" in x.lower() or \"inactive\" in x.lower()]\n",
    "            if i == 0:\n",
    "                poi_info = text[dob_index[0]:closed_indices[0]]\n",
    "                # Name / DOB / SEX\n",
    "                poi_info_l0 = poi_info[0].split(\"DOB:\")\n",
    "                info_dict['Name'] = poi_info_l0[0].strip()\n",
    "                poi_info_l00 = poi_info_l0[1].split(\"Sex:\")\n",
    "                info_dict['DOB'] = poi_info_l00[0].strip()\n",
    "                info_dict['Sex'] = poi_info_l00[1].strip()\n",
    "                # LOCATION / BLANK / EYES\n",
    "                poi_info_l1 = poi_info[1].split(\"Eyes:\")\n",
    "                info_dict['Location'] = poi_info_l1[0].strip()\n",
    "                info_dict['Eyes'] = poi_info_l1[1].strip()\n",
    "                # ALIASES (TITLE ) / BLANK /  HAIR\n",
    "                poi_info_l2 = poi_info[2].split(\"Hair:\")\n",
    "                info_dict['Hair'] = poi_info_l1[1].strip()\n",
    "                # # ALIAS / BLANK / RACE\n",
    "                poi_info_l3 = poi_info[3].split(\"Race:\")\n",
    "                info_dict['Aliases'] = poi_info_l3[0].strip()\n",
    "                info_dict['Race'] = poi_info_l3[1].strip()\n",
    "                if len(poi_info) > 4:\n",
    "                    # sometimes empty strings or Court information is parsed from this Alias strategy\n",
    "                    # sometimes Aliases: Name will be included\n",
    "                    pre_cleaned_aliases = [x.strip().split(\":\")[-1] for x in [info_dict[\"Aliases\"]] + poi_info[4:] if 'Court' not in x]\n",
    "                    info_dict[\"Aliases\"] = [x for x in pre_cleaned_aliases if len(x)>2]\n",
    "\n",
    "\n",
    "            case_info = text[closed_indices[0]+1:]\n",
    "            closed_endings = [0] + give_indices('Proc',case_info)\n",
    "            printed_endings = give_indices('Printed',case_info)\n",
    "\n",
    "            start_index = 0\n",
    "            if len(closed_endings) < 2:\n",
    "                closed_endings.extend(give_indices('Recent entries',case_info))\n",
    "            end_index = closed_endings[1]\n",
    "\n",
    "            case_numbers = []\n",
    "            case_information = []\n",
    "            while len(closed_endings) > 0:\n",
    "            # processing cases \n",
    "                \n",
    "                start_index = closed_endings[0]\n",
    "                if len(closed_endings) > 1:\n",
    "                    end_index = closed_endings[1]-1\n",
    "                else:\n",
    "                    end_index = printed_endings[0]\n",
    "\n",
    "                case_sliced = case_info[start_index:end_index]\n",
    "\n",
    "                ## ripping out case number\n",
    "                # '        MJ-26303-CR-0000170-2003 Processing Status: Completed OTN: H 647213-0       ',\n",
    "                processing_status_str = 'Processing Status:'\n",
    "                proc_status_str = 'Proc Status:'\n",
    "\n",
    "                if any(filter(lambda s: processing_status_str in s, case_sliced)):\n",
    "                    case_number_str = [x for x in case_sliced if processing_status_str in x]\n",
    "                    case_number_index = give_indices(processing_status_str,case_number_str)[0]+1\n",
    "                    case_number = case_number_str[0].split(processing_status_str)[0].strip()\n",
    "                    \n",
    "                elif any(filter(lambda s: proc_status_str in s, case_sliced)):\n",
    "                    case_number_str = [x for x in case_sliced if proc_status_str in x]\n",
    "                    case_number_index = give_indices(proc_status_str,case_number_str)[0]+1\n",
    "                    case_number = case_number_str[0].split(proc_status_str)[0].strip()\n",
    "                else:\n",
    "                    # print(\"No Processing Found\")\n",
    "                    break\n",
    "                \n",
    "\n",
    "                # continue processing the rest of the case info\n",
    "                case_sliced = case_sliced[case_number_index:]\n",
    "\n",
    "                \n",
    "\n",
    "                #  '        Statute          Grade Description             Disposition          Counts  ',\n",
    "                column_names_cases = give_indices('Statute',case_sliced)[0]+1\n",
    "\n",
    "                # Hacky Way to get Description Column\n",
    "                # 1. Determine at what index Description occurs\n",
    "                description_index = case_sliced[column_names_cases-1].index(\"Description\")\n",
    "                \n",
    "\n",
    "                case_info_dict = {}\n",
    "                sliced_case_v0 = case_sliced[:column_names_cases-1]\n",
    "\n",
    "                for x in sliced_case_v0:\n",
    "                    pre_split_v0 = x.split(\"  \")\n",
    "                    pre_split_ws_removed_v0 = [s for s in pre_split_v0 if s]\n",
    "                    for val in pre_split_ws_removed_v0:\n",
    "                        vals = val.split(\":\")\n",
    "                        if len(vals) > 1:\n",
    "                            case_info_dict[vals[0].strip()] = vals[1]\n",
    "\n",
    "                sliced_case_v1 = case_sliced[column_names_cases:]\n",
    "\n",
    "                stat = []\n",
    "                gd = []\n",
    "                counts = []\n",
    "                description = []\n",
    "                disposition = []\n",
    "                for x in sliced_case_v1:\n",
    "                    pre_split = x.split(\"  \")\n",
    "                    pre_split_ws_removed = [s for s in pre_split if s]\n",
    "                    if len(pre_split_ws_removed) > 3:\n",
    "                        stat.append(pre_split_ws_removed[0].strip())\n",
    "                        gd.append(pre_split_ws_removed[1].strip())\n",
    "                        # Descriptions generally follow the same place as the Column\n",
    "                        description.append(pre_split_ws_removed[2].strip())\n",
    "                        disposition.append(pre_split_ws_removed[3].strip())\n",
    "                        counts.append(pre_split_ws_removed[-1].strip())\n",
    "                        \n",
    "                case_info_dict['Statute'] = stat\n",
    "                case_info_dict['Grade Description'] = gd\n",
    "                case_info_dict['Counts'] = counts\n",
    "                case_info_dict['Description'] = description\n",
    "                case_info_dict['Disposition'] = disposition\n",
    "                case_info_dict['Status'] = text[closed_indices[0]].strip()\n",
    "                case_info_dict['case_number'] = case_number\n",
    "\n",
    "                # info_dict[case_number] = case_info_dict\n",
    "                case_numbers.append(case_number)\n",
    "                case_information.append(case_info_dict)\n",
    "\n",
    "                closed_endings.pop(0)\n",
    "\n",
    "        \"\"\"\n",
    "        Construct a POI table where POI information is repeated for each unique case\n",
    "        \"\"\"\n",
    "        df_info = pd.json_normalize(info_dict)\n",
    "        df_case = pd.DataFrame(np.tile(df_info,(len(set(case_numbers)),1)))\n",
    "        df_case['case_number'] = list(set(case_numbers))\n",
    "\n",
    "        \"\"\"\n",
    "        Merge POI table with case information\n",
    "        \"\"\"\n",
    "\n",
    "        df_final = pd.merge(df_case,pd.DataFrame(case_information),how='outer',left_on='case_number',right_on='case_number')\n",
    "        \n",
    "        file_name = cs_file.replace(\".pdf\",\"\")\n",
    "        df_final.to_csv(f\"{output_directory}/{file_name}.csv\")\n",
    "\n",
    "        with open(\"parsed_pdfs.txt\", \"a+\") as file:\n",
    "            file.write(cs_file)\n",
    "\n",
    "        good_files_counter += 1\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        with open(\"tricky_pdfs.txt\", \"a+\") as file:\n",
    "            file.write(cs_file)\n",
    "\n",
    "        tricky_files_counter += 1\n",
    "end_time = time.time()\n",
    "print(f\"{len(court_summaries)} files took {end_time-start_time} seconds to process\")\n",
    "print(f\"{good_files_counter} ({good_files_counter/len(court_summaries):2f}%) were successfully extracted\")\n",
    "print(f\"{tricky_files_counter} ({tricky_files_counter/len(court_summaries):2f}%) were not succesfully extracted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secret_lives_pa",
   "language": "python",
   "name": "secret_lives_pa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
